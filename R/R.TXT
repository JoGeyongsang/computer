library(ggplot2)
library(wordcloud)
library(SnowballC)
library(dplyr)
library(tm)


global = read.csv('C:\\Users\\user\\Desktop\\bigdata wldnjs\\r\\netflixglobal.csv')

global_1=global %>%
subset(weekly_rank==1)%>%
group_by(show_title) %>%
dplyr::summarise(MEAN=mean(weekly_hours_viewed), N=length(weekly_hours_viewed))
global_1

# A tibble: 71 x 3
   show_title                    MEAN     N
   <chr>                        <dbl> <int>
 1 Aftermath                 24200000     1
 2 All of Us Are Dead       124790000     1
 3 Anonymously Yours         12020000     1
 4 Arcane                    38420000     1
 5 Army of Thieves           60625000     2
 6 Black Island              11950000     2
 7 Blood Red Sky             44150000     2
 8 Brazen                    45340000     1
 9 Caf챕 con aroma de mujer  92730000     3
10 Clickbait                 63910000     2
# ... with 61 more rows

global_1 %>%
ggplot()+
geom_bar(aes(x=show_title,y=MEAN,fill=show_title), stat="identity") +
geom_text(aes(x=show_title,y= MEAN+0.05, label=round(MEAN,2)))+
ggtitle("1위 평균시청시간")+
theme(axis.text.x = element_text(angle = 90, size = 8.5,color = "black"))

global = read.csv('C:\\Users\\user\\Desktop\\bigdata wldnjs\\r\\netflixglobal.csv')

library(tm)
CORPUS=Corpus(VectorSource(global$show_title))
CORPUS_TM=tm_map(CORPUS,stripWhitespace)
CORPUS_TM=tm_map(CORPUS_TM,removePunctuation)
CORPUS_TM=tm_map(CORPUS_TM,removeNumbers)
CORPUS_TM=tm_map(CORPUS_TM,tolower)
DTM=DocumentTermMatrix(CORPUS_TM)
inspect(DTM)


<<DocumentTermMatrix (documents: 1240, terms: 703)>>
Non-/sparse entries: 2882/868838
Sparsity           : 100%
Maximal term length: 13
Weighting          : term frequency (tf)
Sample             :
      Terms
Docs   and christmas flow heist love money newly queen red the
  1168   0         0    0     0    0     0     0     0   0   1
  189    0         0    0     0    0     0     0     0   0   1
  22     0         0    0     0    0     0     0     0   0   5
  365    0         0    0     0    0     0     0     0   0   2
  402    0         0    0     0    0     0     0     0   0   2
  606    0         0    0     0    0     0     0     0   0   2
  648    0         0    0     0    0     0     0     0   0   2
  652    0         0    0     0    0     0     0     0   0   1
  694    0         0    0     0    0     0     0     0   0   1
  859    0         0    0     0    0     0     0     0   0   0


CORPUS_TM = tm_map(CORPUS_TM,removeWords, c(stopwords("english"),"and","the","from","with","for"))

TDM=TermDocumentMatrix(CORPUS_TM)
TDM
m=as.matrix(TDM)
v=sort(rowSums(m),decreasing=TRUE)
d=data.frame(word=names(v),freq=v)

wordcloud(words=d$word,freq=d$freq,min.freq=5,max.words=200,random.order=FALSE,colors=brewer.pal(8,"Dark2"))

scala> import org.apache.spark._
import org.apache.spark._

scala> import org.apache.spark.straming.)
<console>:1: error: identifier expected but ')' found.
import org.apache.spark.straming.)
                                 ^

scala> import org.apache.spark.straming._
<console>:26: error: object straming is not a member of package org.apache.spark
       import org.apache.spark.straming._
                               ^

scala> import org.apache.spark.streaming._
import org.apache.spark.streaming._

scala> val ssc=new StreamingContext(sc,Seconds(5))
ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@2086d469

scala> val conf = new SparkConf().setMaster("local[4]").setAppName("App name")
conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@101c15ad

scala> val ssc= new StreamingContext9conf, seconds(5))
<console>:1: error: ';' expected but ',' found.
val ssc= new StreamingContext9conf, seconds(5))
                                  ^

scala> val ssc= new StreamingContext(conf, seconds(5))
<console>:31: error: not found: value seconds
       val ssc= new StreamingContext(conf, seconds(5))
                                           ^

scala> val ssc= new StreamingContext(conf,Seconds(5))
org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:823)
org.apache.spark.repl.Main$.createSparkSession(Main.scala:95)
<init>(<console>:15)
<init>(<console>:31)
<init>(<console>:33)
.<init>(<console>:37)
.<clinit>(<console>)
.$print$lzycompute(<console>:7)
.$print(<console>:6)
$print(<console>)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)
scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)
scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)
scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)
scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2221)
  at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2217)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2217)
  at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2290)
  at org.apache.spark.SparkContext.<init>(SparkContext.scala:89)
  at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:836)
  at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
  ... 52 elided

scala>
[3]+  Stopped                 spark-shell --master local[4]
spark@spark-in-action:~$ CD FIRST-EDTION/CH06
CD: command not found
spark@spark-in-action:~$ cd first-edition/ch06
spark@spark-in-action:~/first-edition/ch06$ tar xvfz orders.tar.gz
orders.txt
spark@spark-in-action:~/first-edition/ch06$ cd home
-bash: cd: home: No such file or directory
spark@spark-in-action:~/first-edition/ch06$ cd spark-in-action
-bash: cd: spark-in-action: No such file or directory
spark@spark-in-action:~/first-edition/ch06$ spark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
Spark context Web UI available at http://10.0.2.15:4043
Spark context available as 'sc' (master = local[*], app id = local-1644306430259).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.0
      /_/

Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_72-internal)
Type in expressions to have them evaluated.
Type :help for more information.

scala>
[4]+  Stopped                 spark-shell
spark@spark-in-action:~/first-edition/ch06$ cd
spark@spark-in-action:~$ spark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
Spark context Web UI available at http://10.0.2.15:4044
Spark context available as 'sc' (master = local[*], app id = local-1644306516573).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.0
      /_/

Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_72-internal)
Type in expressions to have them evaluated.
Type :help for more information.

scala> cat orders.txt | more
<console>:1: error: ';' expected but '.' found.
cat orders.txt | more
          ^

scala> val filestream=ssc.textFileStream("/home/spark/ch06input")
<console>:23: error: not found: value ssc
       val filestream=ssc.textFileStream("/home/spark/ch06input")
                      ^

scala> val filestream=sc.textFileStream("/home/spark/ch06input")
<console>:24: error: value textFileStream is not a member of org.apache.spark.SparkContext
       val filestream=sc.textFileStream("/home/spark/ch06input")
                         ^

scala> val filestream=ssc.textFileStream("/home/spark/ch06input")
<console>:23: error: not found: value ssc
       val filestream=ssc.textFileStream("/home/spark/ch06input")
                      ^

scala> val filestream = ssc.textFileStream("/home/spark/ch06input")
<console>:23: error: not found: value ssc
       val filestream = ssc.textFileStream("/home/spark/ch06input")
                        ^

scala>

scala>





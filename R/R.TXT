library(ggplot2)
library(wordcloud)
library(SnowballC)
library(dplyr)
library(tm)


global = read.csv('C:\\Users\\user\\Desktop\\bigdata wldnjs\\r\\netflixglobal.csv')

global_1=global %>%
subset(weekly_rank==1)%>%
group_by(show_title) %>%
dplyr::summarise(MEAN=mean(weekly_hours_viewed), N=length(weekly_hours_viewed))
global_1

# A tibble: 71 x 3
   show_title                    MEAN     N
   <chr>                        <dbl> <int>
 1 Aftermath                 24200000     1
 2 All of Us Are Dead       124790000     1
 3 Anonymously Yours         12020000     1
 4 Arcane                    38420000     1
 5 Army of Thieves           60625000     2
 6 Black Island              11950000     2
 7 Blood Red Sky             44150000     2
 8 Brazen                    45340000     1
 9 Caf챕 con aroma de mujer  92730000     3
10 Clickbait                 63910000     2
# ... with 61 more rows

global_1 %>%
ggplot()+
geom_bar(aes(x=show_title,y=MEAN,fill=show_title), stat="identity") +
geom_text(aes(x=show_title,y= MEAN+0.05, label=round(MEAN,2)))+
ggtitle("1위 평균시청시간")+
theme(axis.text.x = element_text(angle = 90, size = 8.5,color = "black"))

global = read.csv('C:\\Users\\user\\Desktop\\bigdata wldnjs\\r\\netflixglobal.csv')

library(tm)
CORPUS=Corpus(VectorSource(global$show_title))
CORPUS_TM=tm_map(CORPUS,stripWhitespace)
CORPUS_TM=tm_map(CORPUS_TM,removePunctuation)
CORPUS_TM=tm_map(CORPUS_TM,removeNumbers)
CORPUS_TM=tm_map(CORPUS_TM,tolower)
DTM=DocumentTermMatrix(CORPUS_TM)
inspect(DTM)


<<DocumentTermMatrix (documents: 1240, terms: 703)>>
Non-/sparse entries: 2882/868838
Sparsity           : 100%
Maximal term length: 13
Weighting          : term frequency (tf)
Sample             :
      Terms
Docs   and christmas flow heist love money newly queen red the
  1168   0         0    0     0    0     0     0     0   0   1
  189    0         0    0     0    0     0     0     0   0   1
  22     0         0    0     0    0     0     0     0   0   5
  365    0         0    0     0    0     0     0     0   0   2
  402    0         0    0     0    0     0     0     0   0   2
  606    0         0    0     0    0     0     0     0   0   2
  648    0         0    0     0    0     0     0     0   0   2
  652    0         0    0     0    0     0     0     0   0   1
  694    0         0    0     0    0     0     0     0   0   1
  859    0         0    0     0    0     0     0     0   0   0


CORPUS_TM = tm_map(CORPUS_TM,removeWords, c(stopwords("english"),"and","the","from","with","for"))

TDM=TermDocumentMatrix(CORPUS_TM)
TDM
m=as.matrix(TDM)
v=sort(rowSums(m),decreasing=TRUE)
d=data.frame(word=names(v),freq=v)

wordcloud(words=d$word,freq=d$freq,min.freq=5,max.words=200,random.order=FALSE,colors=brewer.pal(8,"Dark2"))





